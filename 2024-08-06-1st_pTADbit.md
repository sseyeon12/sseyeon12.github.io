# Studying by Following Papers (pTADbit)


```python
class pTADbitDNN3(nn.Module):
    def __init__(self):
        super(pTADbitDNN3, self).__init__()
        self.fc1 = nn.Linear(in_features=361, out_features=64)
        self.fc2 = nn.Linear(in_features=64, out_features=64)
        self.fc3 = nn.Linear(in_features=64, out_features=32)
        self.fc4 = nn.Linear(in_features=32, out_features=1)
        self.fc5 = nn.Linear(in_features=32, out_features=1)
        self.fc6 = nn.Linear(in_features=32, out_features=1)
        self.drop = nn.Dropout(p=0.25)

    def forward(self, x):
        x = F.relu(self.fc1(x))
        #x = self.drop(x)
        x = F.relu(self.fc2(x))
        #x = self.drop(x)
        x = F.relu(self.fc3(x))
        k = F.softplus(self.fc4(x))
        loc = F.softplus(self.fc5(x))
        scale = F.softplus(self.fc6(x))
        return k, loc, scale
```


```python
loss_list = []
outputs_list = []
input_list = []
loss_k = []
loss_loc = []
loss_scale = []

# setting seed
def set_seed(seed=2024):
    '''Sets the seed of the entire notebook so results are the same every time we run.
    This is for REPRODUCIBILITY.'''
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    # When running on the CuDNN backend, two further options must be set
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    # Set a fixed value for the hash seed
    os.environ['PYTHONHASHSEED'] = str(seed)

set_seed(seed=2024)

for epoch in range(50):
    epoch_loss_k = 0.0
    epoch_loss_loc = 0.0
    epoch_loss_scale = 0.0
    batch_count = 0  

    for images, labels in train_loader:
        # images & labels --> tensors
        if isinstance(images, list):
            images = torch.tensor(images, dtype=torch.float32)
        if isinstance(labels, list):
            labels = torch.tensor(labels, dtype=torch.float32)
        
        images, labels = images.to(device), labels.to(device)
        
        # three parameters separate
        individual_labels = torch.unbind(labels)
        input_list.append(individual_labels)
        
        # remove tuple
        K, LOC, SCALE = individual_labels
        
        # dimension changing
        images = images.view(-1, 361)
        
        # model prediction & criterion
        outputs = model3(images)
        
        # model k, loc, scale
        k, loc, scale = outputs
        
        k_loss = criterion(k, K)
        loc_loss = criterion(loc, LOC)
        scale_loss = criterion(scale, SCALE)
        
        # save loss
        loss_k.append(k_loss)
        loss_loc.append(loc_loss)
        loss_scale.append(scale_loss)
        
        
        # total loss
        total_loss = k_loss + loc_loss + scale_loss
        
        # backpropagation
        optimizer.zero_grad()  # initialize weight
        total_loss.backward()
        optimizer.step()
        
        # output save
        outputs_list.append(outputs)
        
        # k
        epoch_loss_k += k_loss.item()
        
        # loc
        epoch_loss_loc += loc_loss.item()
        
        # scale
        epoch_loss_scale += scale_loss.item()
        
        batch_count += 1
    
    epoch_loss_k /= batch_count
    epoch_loss_loc /= batch_count
    epoch_loss_scale /= batch_count
    
    loss_list.append((epoch_loss_k, epoch_loss_loc, epoch_loss_scale))
    
    print(f'Epoch [{epoch+1}/{50}], k_Loss: {epoch_loss_k:.4f}, loc_Loss: {epoch_loss_loc:.4f}, scale_Loss: {epoch_loss_scale:.4f}')
```

    Epoch [1/50], k_Loss: 18.1641, loc_Loss: 162003.2560, scale_Loss: 46321.3645
    Epoch [2/50], k_Loss: 7.9917, loc_Loss: 20973.4476, scale_Loss: 5978.6900
    Epoch [3/50], k_Loss: 9.8956, loc_Loss: 28612.5765, scale_Loss: 19994.9937
    Epoch [4/50], k_Loss: 5.9271, loc_Loss: 21659.9937, scale_Loss: 11874.8624
    Epoch [5/50], k_Loss: 61.8680, loc_Loss: 15267.6850, scale_Loss: 4639.6554
    Epoch [6/50], k_Loss: 7.6853, loc_Loss: 12499.7323, scale_Loss: 3194.1819
    Epoch [7/50], k_Loss: 7.8347, loc_Loss: 10881.9077, scale_Loss: 3567.7632
    Epoch [8/50], k_Loss: 7.9911, loc_Loss: 13609.4246, scale_Loss: 3482.1939
    Epoch [9/50], k_Loss: 7.7352, loc_Loss: 11614.1323, scale_Loss: 3165.2919
    Epoch [10/50], k_Loss: 7.6184, loc_Loss: 15269.0404, scale_Loss: 2763.4680
    Epoch [11/50], k_Loss: 7.6351, loc_Loss: 11489.3306, scale_Loss: 2999.9413
    Epoch [12/50], k_Loss: 7.5080, loc_Loss: 12836.7069, scale_Loss: 3168.7514
    Epoch [13/50], k_Loss: 7.9026, loc_Loss: 10604.9846, scale_Loss: 2258.4624
    Epoch [14/50], k_Loss: 7.4583, loc_Loss: 10233.8229, scale_Loss: 2190.2300
    Epoch [15/50], k_Loss: 6.1265, loc_Loss: 8631.9788, scale_Loss: 1864.1043
    Epoch [16/50], k_Loss: 8.0441, loc_Loss: 7884.4222, scale_Loss: 1933.6619
    Epoch [17/50], k_Loss: 7.3848, loc_Loss: 12570.6327, scale_Loss: 2695.8137
    Epoch [18/50], k_Loss: 7.2629, loc_Loss: 7592.7404, scale_Loss: 1613.9076
    Epoch [19/50], k_Loss: 6.4356, loc_Loss: 7383.7027, scale_Loss: 1565.3083
    Epoch [20/50], k_Loss: 6.6944, loc_Loss: 6959.4708, scale_Loss: 1442.2014
    Epoch [21/50], k_Loss: 6.2861, loc_Loss: 6396.8959, scale_Loss: 1303.9339
    Epoch [22/50], k_Loss: 6.7627, loc_Loss: 7007.2437, scale_Loss: 1448.7961
    Epoch [23/50], k_Loss: 7.4534, loc_Loss: 6574.6168, scale_Loss: 1384.2170
    Epoch [24/50], k_Loss: 7.4152, loc_Loss: 6563.2501, scale_Loss: 1365.2496
    Epoch [25/50], k_Loss: 6.3032, loc_Loss: 5915.9402, scale_Loss: 1252.8390
    Epoch [26/50], k_Loss: 4.9446, loc_Loss: 5863.0678, scale_Loss: 1247.2218
    Epoch [27/50], k_Loss: 4.3399, loc_Loss: 5023.6030, scale_Loss: 1092.3478
    Epoch [28/50], k_Loss: 3.6612, loc_Loss: 4540.7671, scale_Loss: 966.5829
    Epoch [29/50], k_Loss: 5.5708, loc_Loss: 5012.7556, scale_Loss: 1053.5634
    Epoch [30/50], k_Loss: 5.1299, loc_Loss: 5071.7917, scale_Loss: 1077.1038
    Epoch [31/50], k_Loss: 4.6630, loc_Loss: 4521.8042, scale_Loss: 969.8356
    Epoch [32/50], k_Loss: 4.4330, loc_Loss: 4789.3403, scale_Loss: 1012.8363
    Epoch [33/50], k_Loss: 5.0912, loc_Loss: 4441.4923, scale_Loss: 903.4059
    Epoch [34/50], k_Loss: 4.8089, loc_Loss: 4652.9929, scale_Loss: 1011.3227
    Epoch [35/50], k_Loss: 4.0969, loc_Loss: 5198.3064, scale_Loss: 1029.4282
    Epoch [36/50], k_Loss: 5.6744, loc_Loss: 4435.6671, scale_Loss: 965.0283
    Epoch [37/50], k_Loss: 4.2516, loc_Loss: 5449.6327, scale_Loss: 1213.5711
    Epoch [38/50], k_Loss: 4.7833, loc_Loss: 4635.5269, scale_Loss: 1046.5785
    Epoch [39/50], k_Loss: 5.6926, loc_Loss: 4544.7801, scale_Loss: 986.1315
    Epoch [40/50], k_Loss: 4.2930, loc_Loss: 4337.0916, scale_Loss: 942.9164
    Epoch [41/50], k_Loss: 4.3195, loc_Loss: 4249.2162, scale_Loss: 969.5378
    Epoch [42/50], k_Loss: 4.1108, loc_Loss: 4022.9922, scale_Loss: 906.9152
    Epoch [43/50], k_Loss: 4.4354, loc_Loss: 4370.2725, scale_Loss: 952.0288
    Epoch [44/50], k_Loss: 6.3588, loc_Loss: 4762.8562, scale_Loss: 1052.7202
    Epoch [45/50], k_Loss: 18.1911, loc_Loss: 3997.5168, scale_Loss: 845.0055
    Epoch [46/50], k_Loss: 5.9711, loc_Loss: 4134.1388, scale_Loss: 891.9953
    Epoch [47/50], k_Loss: 20.4952, loc_Loss: 4038.2278, scale_Loss: 889.1279
    Epoch [48/50], k_Loss: 6.5654, loc_Loss: 3880.8873, scale_Loss: 823.7358
    Epoch [49/50], k_Loss: 6.2564, loc_Loss: 3917.2809, scale_Loss: 871.9249
    Epoch [50/50], k_Loss: 6.8287, loc_Loss: 3884.3688, scale_Loss: 826.0797



```python
sns.lmplot(x="obseved_K", y="predicted_K", data=labels_in_out);
```


    
![png](output_3_0.png)
    



```python
stats.pearsonr(labels_in_out['obseved_K'], labels_in_out['predicted_K'])
```




    PearsonRResult(statistic=0.5418242999662517, pvalue=0.0)




```python
sns.lmplot(x="obseved_loc", y="predicted_loc", data=labels_in_out);
```


    
![png](output_5_0.png)
    



```python
stats.pearsonr(labels_in_out['obseved_loc'], labels_in_out['predicted_loc'])
```




    PearsonRResult(statistic=0.8837372970181123, pvalue=0.0)




```python
sns.lmplot(x="obseved_scale", y="predicted_scale", data=labels_in_out);
```


    
![png](output_7_0.png)
    



```python
stats.pearsonr(labels_in_out['obseved_scale'], labels_in_out['predicted_scale'])
```




    PearsonRResult(statistic=0.8738117183200163, pvalue=0.0)




```python
# k loss value
epochs = list(range(1, len(k_loss_value) + 1))

plt.figure(figsize=(10, 5))
plt.plot(epochs, k_loss_value, marker='o', linestyle='-', color='b', label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Epoch vs. Loss')
plt.legend()
plt.grid(True)
plt.show()
```


    
![png](output_9_0.png)
    



```python
# loc loss value
epochs = list(range(1, len(loc_loss_value) + 1))

plt.figure(figsize=(10, 5))
plt.plot(epochs, loc_loss_value, marker='o', linestyle='-', color='b', label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Epoch vs. Loss')
plt.legend()
plt.grid(True)
plt.show()
```


    
![png](output_10_0.png)
    



```python
# scale loss value
epochs = list(range(1, len(scale_loss_value) + 1))

plt.figure(figsize=(10, 5))
plt.plot(epochs, scale_loss_value, marker='o', linestyle='-', color='b', label='Training Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss')
plt.title('Epoch vs. Loss')
plt.legend()
plt.grid(True)
plt.show()
```


    
![png](output_11_0.png)
    

